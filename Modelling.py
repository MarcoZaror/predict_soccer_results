# -*- coding: utf-8 -*-
"""
Created on Mon May 27 10:08:26 2019

@author: mazaror
Modelling
"""

#Import base created in ETL_2
tablon = pd.read_pickle('C:/Users/mazaror/Documents/Respaldo Maro Zaror/Personal/Python/Proyectos/Futbol/base_v2.pkl')

#Check for null values
tablon.isnull().any()
tablon3[tablon3['ptos_h_last_season'].isnull() == True]

# generates train and test databases for cross-validation
test = tablon[tablon['Season'] == 'T 18-19']
train = tablon[tablon['Season'] != 'T 18-19']

X_train = train.iloc[:,4:]
y_train = train.loc[:,'FTR']

X_test = test.iloc[:,4:]
y_test = test.loc[:,'FTR']

# Normalization and scaling some variables
cols_scal= ['dif_locloc_visvis','dif_gol','difgol_locloc_visvis','dif_ptos_ls','rat_inv_2ls']
cols_minmax_scal=['pbb_loc','pbb_vis']

scaler = StandardScaler()
minmax = MinMaxScaler()

scaler.fit(tablon[cols_scal])
minmax.fit(tablon[cols_minmax_scal])

X_train[cols_scal] = scaler.transform(X_train[cols_scal])
X_train[cols_minmax_scal] = minmax.transform(X_train[cols_minmax_scal])


X_test[cols_scal] = scaler.transform(X_test[cols_scal])
X_test[cols_minmax_scal] = minmax.transform(X_test[cols_minmax_scal])

#Predicting 3 classes (Home, Tie or Away) an SVM model achieves 58.5% of accuracy
#Without considering the first 3 matches of each team (not enough information), accuracy reaches 61,5%

svm = SVC(probability=True)
svm.fit(X_train,y_train)

y_pred = svm.predict(X_test)
confusion_matrix(y_test,y_pred)
accuracy_score(y_test,y_pred)

#Using now the confidence of the model, accuracies of 66% and 69% can be reached
probs = svm.predict_proba(X_test)
probs = pd.DataFrame(probs)
X_test['real'] = y_test
X_test['pred'] = y_pred
X_test['pA'] = probs[0].values
X_test['pD'] = probs[1].values
cant_part = tablon['cant_jgos_h'][(tablon['Season'] == 'T 18-19')]
X_test['pH'] = probs[2].values

X_test['cant_part'] = cant_part

X_test['acc'] = X_test.apply(lambda x: 1 if x['real'] == x['pred'] else 0, axis=1)
X_test['clas_part'] = X_test.apply(lambda x: '1-15' if x['cant_part'] <= 15 else '16-30', axis= 1 )
X_test['pH > 55'] = X_test.apply(lambda x: 1 if x['pH'] > 0.55 else 0, axis=1)
X_test['pA > 50'] = X_test.apply(lambda x: 1 if x['pA'] > 0.50 else 0, axis=1)        

X_test['pH > 55'].value_counts()
X_test['pA > 50'].value_counts()

X_test[['pH > 55', 'acc']].groupby('pH > 55').mean() # 66% accuracy H
X_test[['pA > 50', 'acc']].groupby('pA > 50').mean() # 69% accuracy A
X_test['apuesta_seg'] = X_test.apply(lambda x: 1 if x['pH > 55'] == 1 else (1 if x['pA > 50'] == 1 else 0), axis=1)
X_test['apuesta_seg'].value_counts()
X_test[['apuesta_seg', 'acc']].groupby('apuesta_seg').mean()

#Comparing now with bet365 factors, it can be seen that they are really similar to the ones generated by the model
b365_1819 = pd.read_csv('C:/Users/mazaror/Documents/Respaldo Maro Zaror/Personal/Python/Proyectos/Futbol/b35418_19.csv', sep='\t')

X_test = X_test.drop(['pH > 70','pA > 70','pH > 60','pA > 60','pH > 50','pA > 55'], axis=1)
X_test = X_test.reset_index()
X_test['b365_H'] = b365_1819['B365H'] 
X_test['b365_D'] = b365_1819['B365D'] 
X_test['b365_A'] = b365_1819['B365A'] 

X_test['bet365HDA'] = X_test.apply(lambda x: x['b365_H'] if x['pred'] == 'H' else (x['b365_A'] if x['pred'] == 'A' else x['b365_D']), axis= 1)
X_test['bet365HDA'][X_test['apuesta_seg']== 1]

X_test['monto'] = X_test.apply(lambda x: x['bet365HDA']*x['acc']*1000, axis=1)

#EvaluaciÃ³n negocios
X_test['monto'].sum() #Gano 348.000 apostando 340.000
X_test[X_test['apuesta_seg'] == 1]['monto'].sum() #Gano 197.000 apostando 190.000

X_test['cuota_H'] = 1/X_test['pH']
X_test['cuota_D'] = 1/X_test['pD']
X_test['cuota_A'] = 1/X_test['pA']

X_test['dif_cuota'] = X_test.apply(lambda x: x['cuota_H'] - x['b365_H'] if x['pred'] == 'H' else (x['cuota_A'] - x['b365_A'] if x['pred'] == 'A' else x['cuota_D'] - x['b365_D']), axis= 1)


X_test['cta_fav'] = X_test.apply(lambda x: 1 if x['dif_cuota'] > 0.3 else 0, axis=1)
X_test[X_test['cta_fav'] == 1]['monto'].sum() #gano 133.000 si apuesto a 124 partidos
X_test['cta_fav'].value_counts() 

#Using more advance models doesn't show an improvement, probably because of the lack of data
from sklearn.ensemble import GradientBoostingClassifier

GBC = GradientBoostingClassifier(max_depth = 3, n_estimators=500, learning_rate=1.0)
GBC.fit(X_train,y_train)

y_pred = GBC.predict(X_test)
confusion_matrix(y_test,y_pred)
accuracy_score(y_test,y_pred)

from sklearn.neural_network import MLPClassifier

mlp = MLPClassifier(early_stopping=True)
mlp.fit(X_train,y_train)

y_pred = mlp.predict(X_test)
confusion_matrix(y_test,y_pred)
accuracy_score(y_test,y_pred)



###################################################Modelamiento binario

tablon4 = pd.get_dummies(tablon, columns=['FTR'])
tablon3.info()
columns_h= ['FTR_H','dif_ptos','dif_locloc_visvis','dif_gol','difgol_locloc_visvis',
               'ult_res_ex','dif_ptos_ls','dif_inv_jug','dif_bal','dif_iv_x_jug']

tablon4 = tablon4[columns_h]
tablon3.boxplot('dif_ptos', by= 'FTR_H')

tablon3['dif_ptos_may5'] = tablon3['dif_ptos'].apply(lambda x: 1 if x >=5 else 0)

tablon3.hist(column='dif_ptos', by='FTR_H', bins=20)

tablon4 = tablon4[(tablon4['iter_fecha'] != 1)]
tablon4 = tablon4[(tablon4['iter_fecha'] != 2)]
tablon4 = tablon4[(tablon4['iter_fecha'] != 3)]


test = tablon4[tablon4['Season']== 'T 18-19']
train = tablon4[tablon4['Season'] != 'T 18-19']



test = test[columns_h]
train = train[columns_h]

X_train = train.iloc[:,1:]
y_train = train.iloc[:,0]

x_test = test.iloc[:,1:]
y_test = test.iloc[:,0]

scaler = StandardScaler()
scaler.fit(X_train)
X_train = scaler.transform(X_train)
x_test = scaler.transform(x_test)

rclf = RandomForestClassifier()
rclf.fit(X_train,y_train)

y_pred = rclf.predict(x_test)
confusion_matrix(y_test,y_pred)
accuracy_score(y_test,y_pred)

svm = SVC()
svm.fit(X_train,y_train)

y_pred = svm.predict(x_test)
confusion_matrix(y_test,y_pred)
accuracy_score(y_test,y_pred)
#SVM con data normalizada para predecir hogar - 66% accuracy